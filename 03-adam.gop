package main

import (
	"fmt"
	at "github.com/gotorch/gotorch/aten"
	"github.com/gotorch/gotorch/torch"
	"github.com/gotorch/gotorch/torch/optim"
)

func main() {
	N, D_in, H, D_out := 64, 1000, 100, 10

	x := torch.RandN(N, Din, requires_grad=False)
	y := torch.RandN(N, Dout, requires_grad=False)

	w1 := torch.randn(D_in, H, requires_grad=True)
	w2 := torch.randn(H, D_out, requires_grad=True)

	learning_rate := 1e-3
	adam := optim.NewAdam([w1, w2], lr=learning_rate)

	for i := 0; i < 500; i++ {
		y_pred := at.Sum(at.Clamp(at.MM(x, w1), 0), w2)
		loss := at.Sum(at.Pow(at.Sub(y_pred, y), 2))

		if i%100 == 0 {
			fmt.Println("loss = ", loss)
		}

		adam.ZeroGrad()
		loss.Backward()
		adam.Step()
	}
}
